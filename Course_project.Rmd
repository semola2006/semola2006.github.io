## ASSIGNMENT BACKGROUND:

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set that is a categorical variable with 5 different levels (A, B, C, D, E). 
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 
We will download the train and test datasets from the links you can find below.

The packages we will use are the following:

    * library(RCurl) ; library(caret) ; library(rpart)
    * library(rattle); library(gbm)   ; library(plyr)
    

```{r, include=FALSE}
library(RCurl)
library(caret)
library(rpart)
library(rattle)
library(gbm)
library(plyr)
```

```{r, echo = TRUE, cache =TRUE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

train <- getURL(url_train, ssl.verifypeer = FALSE)
test  <- getURL(url_test, ssl.verifypeer = FALSE)

training <- read.csv(textConnection(train), header = TRUE, na.strings=c("NA","#DIV/0!",""))
testing  <- read.csv(textConnection(test), header = TRUE, na.strings=c("NA","#DIV/0!",""))
```

We approach this problem in four main steps.

  1. Clean the dataset and remove variable with low variability.
  2. Partition our clean dataset into myTraining (75%) and myTesting (25%) for parameters tuning
  3. Train a model for our classification problems and compute the expected out of sample error
  4. Use our model for out of sample validation


```{r, echo = TRUE, cache = TRUE}
# remove varaibles with NAs above 50% threshold and remove first columns

training_1  <- training[, colSums(is.na(training)) < nrow(training) * 0.5]
training_1  <- training_1[ ,-c(1:6)]

# shrink the number of variables with near zero var function

nsv         <- nearZeroVar(training_1, saveMetrics = TRUE)
training_2  <- training_1[c(rownames(nsv[nsv$nzv == "FALSE",])) ]
Train       <- training_2

rm(training_1); rm(training_2) # remove temp dataframes


```

We now partition the clean Training dataset for cross validation process

```{r, echo=TRUE}
# 
set.seed(1234)
inTrain     <- createDataPartition(Train$classe, p=0.75, list = FALSE)
myTraining  <- Train[inTrain, ]
myTesting   <- Train[-inTrain, ]

```

We follow the indications provided in the lessons of this class and decide to use boosting to help us with our classification problem. We fit a boosted tree model using the "gmb" package.
For parameter tuning, 5 fold CV repeated 2 times that we will specify in the trainControl() function.
The tuneGrid() parameters have been selected and set as optimal after a few test runs to reduce the time lapse. 
The code below is largely built following the example on "http://topepo.github.io/caret/training.html"

```{r, cache=TRUE}


set.seed(4321)
fitControl <- trainControl(
              method = "repeatedcv",
              number = 5,
              repeats = 2)

fitGrid   <- expand.grid(interaction.depth = c(1,2,3),
                         n.trees = (1:3)*50,
                         shrinkage = 0.1,
                         n.minobsinnode = 10)
Fit1 <- train(classe ~ ., 
              data = myTraining, 
              method = "gbm", 
              trControl = fitControl,
              verbose = FALSE,
              tuneGrid = fitGrid)
```

we can represent the outcome of our model and then show graphically the resampling profile

```{r, echo = TRUE}

Fit1

plot(Fit1)

plot(Fit1, metric = "Accuracy", plotType = "level",
     scales = list(x = list(rot = 90)))

```

Finally, we can apply to our testing dataset to get an appropriate estimate of the out of sample error. This method is based on the principle that we must always calculate forecast accuracy measures using test data that was not used when computing the model. 

```{r, echo = TRUE}
# Apply to predict on the test dataset
gmb_predict <- predict(Fit1, newdata = myTesting)
confusionMatrix(gmb_predict, myTesting$classe)
```
The accuracy achieved with our boosted tree model is "Accuracy : 0.9874" and therefore the out of sample error is (1- accuracy) = 0.0126 or 1.26%

In conclusion we can apply our model to predict how the participants performed their activity measured in the separate test dataset provided that we already downloaded and called "testing" and print the results of our prediction.

```{r, echo = TRUE}

final <- predict(Fit1, testing)
final

```

